{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI/CD for a Kubeflow pipeline on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "1. Learn how to create a custom Cloud Build builder to pilote Vertex AI Pipelines\n",
    "1. Learn how to write a Cloud Build config file to build and push all the artifacts for a KFP\n",
    "1. Learn how to setup a Cloud Build GitHub trigger a new run of the Kubeflow PIpeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will walk through authoring of a **Cloud Build** CI/CD workflow that automatically builds, deploys, and runs a Kubeflow pipeline on Vertex AI. You will also integrate your workflow with **GitHub** by setting up a trigger that starts the  workflow when a new tag is applied to the **GitHub** repo hosting the pipeline's code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "REGION = \"us-central1\"\n",
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-kfp-artifact-store\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make sure that the artifact store exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-asl-01-1d0b6cba208c-kfp-artifact-store/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the KFP CLI builder for Vertex AI\n",
    "### Review the Dockerfile describing the KFP CLI builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "RUN pip install kfp==1.6.6\n",
      "RUN pip install google-cloud-aiplatform==1.3.0\n",
      "ENTRYPOINT [\"/bin/bash\"]\n"
     ]
    }
   ],
   "source": [
    "!cat kfp-cli_vertex/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2021 Google LLC\n",
      "\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this\n",
      "# file except in compliance with the License. You may obtain a copy of the License at\n",
      "\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\"\n",
      "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
      "# express or implied. See the License for the specific language governing\n",
      "# permissions and limitations under the License.\n",
      "\n",
      "steps:\n",
      "# Build the trainer image\n",
      "- name: 'gcr.io/cloud-builders/docker'\n",
      "  id: 'Build the trainer image'\n",
      "  args: ['build', '-t', 'gcr.io/$PROJECT_ID/trainer_image_covertype_vertex:latest', '.']\n",
      "  dir: $_PIPELINE_FOLDER/trainer_image_vertex\n",
      "\n",
      "\n",
      "# Push the trainer image, to make it available in the compile step\n",
      "- name: 'gcr.io/cloud-builders/docker'\n",
      "  id: 'Push the trainer image'\n",
      "  args: ['push', 'gcr.io/$PROJECT_ID/trainer_image_covertype_vertex:latest']\n",
      "  dir: $_PIPELINE_FOLDER/trainer_image_vertex\n",
      "\n",
      "\n",
      "# Compile the pipeline\n",
      "- name: 'gcr.io/$PROJECT_ID/kfp-cli-vertex'\n",
      "  id: 'Compile the pipeline'\n",
      "  args:\n",
      "  - '-c'\n",
      "  - |\n",
      "    dsl-compile-v2 --py pipeline.py --output covertype_kfp_pipeline.json\n",
      "  env:\n",
      "  - 'PIPELINE_ROOT=gs://$PROJECT_ID-kfp-artifact-store/pipeline'\n",
      "  - 'PROJECT_ID=$PROJECT_ID'\n",
      "  - 'REGION=$_REGION'\n",
      "  - 'SERVING_CONTAINER_IMAGE_URI=us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-20:latest'\n",
      "  - 'TRAINING_CONTAINER_IMAGE_URI=gcr.io/$PROJECT_ID/trainer_image_covertype_vertex:latest'\n",
      "  - 'TRAINING_FILE_PATH=gs://$PROJECT_ID-kfp-artifact-store/data/training/dataset.csv'\n",
      "  - 'VALIDATION_FILE_PATH=gs://$PROJECT_ID-kfp-artifact-store/data/validation/dataset.csv'\n",
      "  dir: $_PIPELINE_FOLDER/pipeline_vertex\n",
      "\n",
      "# Run the pipeline\n",
      "- name: 'gcr.io/$PROJECT_ID/kfp-cli-vertex'\n",
      "  id: 'Run the pipeline'\n",
      "  args:\n",
      "  - '-c'\n",
      "  - |\n",
      "    python $_PIPELINE_FOLDER/kfp-cli_vertex/run_pipeline.py --project_id=$PROJECT_ID --template_path=$_PIPELINE_FOLDER/pipeline_vertex/covertype_kfp_pipeline.json --display_name=coverype_kfp_pipeline --region=$_REGION\n",
      "\n",
      "# Push the images to Container Registry\n",
      "images: ['gcr.io/$PROJECT_ID/trainer_image_covertype_vertex:latest']\n",
      "\n",
      "# This is required since the pipeline run overflows the default timeout\n",
      "timeout: 10800s\n"
     ]
    }
   ],
   "source": [
    "!cat cloudbuild_vertex.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the image and push it to your project's **Container Registry**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/qwiklabs-asl-01-1d0b6cba208c/kfp-cli-vertex:latest'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KFP_CLI_IMAGE_NAME = \"kfp-cli-vertex\"\n",
    "KFP_CLI_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{KFP_CLI_IMAGE_NAME}:latest\"\n",
    "KFP_CLI_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 1.1 KiB before compression.\n",
      "Uploading tarball of [kfp-cli_vertex] to [gs://qwiklabs-asl-01-1d0b6cba208c_cloudbuild/source/1658424347.160552-8b2661b97833426789c258fb5f201c03.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-asl-01-1d0b6cba208c/locations/global/builds/89dc5906-5641-4949-98fd-1de2b15df6cf].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/89dc5906-5641-4949-98fd-1de2b15df6cf?project=40028652130].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"89dc5906-5641-4949-98fd-1de2b15df6cf\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-asl-01-1d0b6cba208c_cloudbuild/source/1658424347.160552-8b2661b97833426789c258fb5f201c03.tgz#1658424347422456\n",
      "Copying gs://qwiklabs-asl-01-1d0b6cba208c_cloudbuild/source/1658424347.160552-8b2661b97833426789c258fb5f201c03.tgz#1658424347422456...\n",
      "/ [1 files][  899.0 B/  899.0 B]                                                \n",
      "Operation completed over 1 objects/899.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  3.584kB\n",
      "Step 1/4 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7bfe07ed847: Pulling fs layer\n",
      "256a16f81350: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "1a9e3557648b: Pulling fs layer\n",
      "b32cfad274a4: Pulling fs layer\n",
      "633b6bf6eafe: Pulling fs layer\n",
      "dc3ebba293d2: Pulling fs layer\n",
      "f14428c2780c: Pulling fs layer\n",
      "17610ff26dd4: Pulling fs layer\n",
      "c59c59a1007a: Pulling fs layer\n",
      "afa44ccc646d: Pulling fs layer\n",
      "3202e8e43491: Pulling fs layer\n",
      "700cfc1617dd: Pulling fs layer\n",
      "610d6205e6a8: Pulling fs layer\n",
      "61b77f764283: Pulling fs layer\n",
      "c891c87c58c1: Pulling fs layer\n",
      "20b2898486c4: Pulling fs layer\n",
      "1242edd5f479: Pulling fs layer\n",
      "1a9e3557648b: Waiting\n",
      "b32cfad274a4: Waiting\n",
      "633b6bf6eafe: Waiting\n",
      "dc3ebba293d2: Waiting\n",
      "f14428c2780c: Waiting\n",
      "17610ff26dd4: Waiting\n",
      "c59c59a1007a: Waiting\n",
      "afa44ccc646d: Waiting\n",
      "3202e8e43491: Waiting\n",
      "700cfc1617dd: Waiting\n",
      "610d6205e6a8: Waiting\n",
      "61b77f764283: Waiting\n",
      "c891c87c58c1: Waiting\n",
      "20b2898486c4: Waiting\n",
      "1242edd5f479: Waiting\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "256a16f81350: Verifying Checksum\n",
      "256a16f81350: Download complete\n",
      "d7bfe07ed847: Verifying Checksum\n",
      "d7bfe07ed847: Download complete\n",
      "633b6bf6eafe: Verifying Checksum\n",
      "633b6bf6eafe: Download complete\n",
      "dc3ebba293d2: Verifying Checksum\n",
      "dc3ebba293d2: Download complete\n",
      "b32cfad274a4: Verifying Checksum\n",
      "b32cfad274a4: Download complete\n",
      "f14428c2780c: Verifying Checksum\n",
      "f14428c2780c: Download complete\n",
      "17610ff26dd4: Verifying Checksum\n",
      "17610ff26dd4: Download complete\n",
      "c59c59a1007a: Verifying Checksum\n",
      "c59c59a1007a: Download complete\n",
      "afa44ccc646d: Verifying Checksum\n",
      "afa44ccc646d: Download complete\n",
      "3202e8e43491: Verifying Checksum\n",
      "3202e8e43491: Download complete\n",
      "700cfc1617dd: Verifying Checksum\n",
      "700cfc1617dd: Download complete\n",
      "610d6205e6a8: Verifying Checksum\n",
      "610d6205e6a8: Download complete\n",
      "61b77f764283: Verifying Checksum\n",
      "61b77f764283: Download complete\n",
      "c891c87c58c1: Verifying Checksum\n",
      "c891c87c58c1: Download complete\n",
      "1242edd5f479: Verifying Checksum\n",
      "1242edd5f479: Download complete\n",
      "1a9e3557648b: Verifying Checksum\n",
      "1a9e3557648b: Download complete\n",
      "d7bfe07ed847: Pull complete\n",
      "256a16f81350: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "20b2898486c4: Verifying Checksum\n",
      "20b2898486c4: Download complete\n",
      "1a9e3557648b: Pull complete\n",
      "b32cfad274a4: Pull complete\n",
      "633b6bf6eafe: Pull complete\n",
      "dc3ebba293d2: Pull complete\n",
      "f14428c2780c: Pull complete\n",
      "17610ff26dd4: Pull complete\n",
      "c59c59a1007a: Pull complete\n",
      "afa44ccc646d: Pull complete\n",
      "3202e8e43491: Pull complete\n",
      "700cfc1617dd: Pull complete\n",
      "610d6205e6a8: Pull complete\n",
      "61b77f764283: Pull complete\n",
      "c891c87c58c1: Pull complete\n",
      "20b2898486c4: Pull complete\n",
      "1242edd5f479: Pull complete\n",
      "Digest: sha256:fd1a6b82332194835f0dd686b8b9e8f8e372919e0e83e6663160b823dd65e194\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 234ba2bc2b77\n",
      "Step 2/4 : RUN pip install kfp==1.6.6\n",
      " ---> Running in f7dd42729af6\n",
      "Collecting kfp==1.6.6\n",
      "  Downloading kfp-1.6.6.tar.gz (224 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.8/224.8 kB 6.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting absl-py<=0.11,>=0.9\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.8/127.8 kB 20.7 MB/s eta 0:00:00\n",
      "Collecting PyYAML<6,>=5.3\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 636.6/636.6 kB 28.9 MB/s eta 0:00:00\n",
      "Collecting google-cloud-storage<2,>=1.20.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.8/106.8 kB 14.3 MB/s eta 0:00:00\n",
      "Collecting kubernetes<13,>=8.0.0\n",
      "  Downloading kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 66.2 MB/s eta 0:00:00\n",
      "Collecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.1/62.1 kB 12.1 MB/s eta 0:00:00\n",
      "Collecting google-auth<2,>=1.6.1\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.9/152.9 kB 24.1 MB/s eta 0:00:00\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.3/54.3 kB 8.2 MB/s eta 0:00:00\n",
      "Collecting cloudpickle<2,>=1.3.0\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.2.tar.gz (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.0/58.0 kB 10.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 kB 10.7 MB/s eta 0:00:00\n",
      "Collecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Collecting click<8,>=7.1.1\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.8/82.8 kB 12.8 MB/s eta 0:00:00\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.14.1-py3-none-any.whl (33 kB)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.8\n",
      "  Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 kB 15.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.6) (3.20.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<=0.11,>=0.9->kfp==1.6.6) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.6.6) (1.14.1)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.6) (0.1.0)\n",
      "Collecting uritemplate<4dev,>=3.0.0\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.6) (2.8.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.6) (0.20.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.6) (0.2.7)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.6) (59.8.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.6) (4.8)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.6.6) (2.28.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.6.6) (2.3.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.6.6) (2.3.3)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.6) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.6) (21.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.6) (4.11.4)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.6) (1.26.9)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.6) (2022.6.15)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.6) (2.8.2)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=8.0.0->kfp==1.6.6) (1.3.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=8.0.0->kfp==1.6.6) (1.3.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.6.6) (0.37.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp==1.6.6) (1.56.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.6) (1.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp==1.6.6) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.6.6) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.6) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.6) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp==1.6.6) (4.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp==1.6.6) (3.8.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<13,>=8.0.0->kfp==1.6.6) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.6) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.6) (2.21)\n",
      "Building wheels for collected packages: kfp, fire, kfp-server-api, strip-hints, termcolor\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.6.6-py3-none-any.whl size=308913 sha256=ea64792b45aa957cc91051fa3a9c8639e45eb88ef0db4ae06b0cb4d9ad039f03\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/ec/07/d8a332e22c1098a45bdd0c54cb0dafd84bad0ca0e4f1b734ae\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=cf6e9f53ca5db63a0aef5df71ddab776753dce78a29b07f23b1f2b5205b404b9\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.2-py3-none-any.whl size=99716 sha256=4c2a2404f6a98dbb812bfc253f114366997de6a05393c99925685bb64a048933\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/36/d3/60e33cc9e15f269fe0e0f71cae6d077a5e43973d514b60b4ad\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=0b0770eb406a3ca254bb3d658ecd4cc2169ee030a5ce5777bc8489e614eaa225\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=b6fc98d6d3b42c2e603b90a4bc94fed15977bb44be9336f47b769dedd4321251\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built kfp fire kfp-server-api strip-hints termcolor\n",
      "Installing collected packages: termcolor, uritemplate, tabulate, strip-hints, PyYAML, kfp-pipeline-spec, fire, docstring-parser, Deprecated, cloudpickle, click, cachetools, absl-py, requests-toolbelt, kfp-server-api, jsonschema, google-auth, kubernetes, google-api-python-client, google-cloud-storage, kfp\n",
      "  Attempting uninstall: uritemplate\n",
      "    Found existing installation: uritemplate 4.1.1\n",
      "    Uninstalling uritemplate-4.1.1:\n",
      "      Successfully uninstalled uritemplate-4.1.1\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.1.0\n",
      "    Uninstalling cloudpickle-2.1.0:\n",
      "      Successfully uninstalled cloudpickle-2.1.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.3\n",
      "    Uninstalling click-8.1.3:\n",
      "      Successfully uninstalled click-8.1.3\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.0.0\n",
      "    Uninstalling cachetools-5.0.0:\n",
      "      Successfully uninstalled cachetools-5.0.0\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.6.1\n",
      "    Uninstalling jsonschema-4.6.1:\n",
      "      Successfully uninstalled jsonschema-4.6.1\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.9.0\n",
      "    Uninstalling google-auth-2.9.0:\n",
      "      Successfully uninstalled google-auth-2.9.0\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 24.2.0\n",
      "    Uninstalling kubernetes-24.2.0:\n",
      "      Successfully uninstalled kubernetes-24.2.0\n",
      "  Attempting uninstall: google-api-python-client\n",
      "    Found existing installation: google-api-python-client 2.52.0\n",
      "    Uninstalling google-api-python-client-2.52.0:\n",
      "      Successfully uninstalled google-api-python-client-2.52.0\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.4.0\n",
      "    Uninstalling google-cloud-storage-2.4.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.4.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "black 22.6.0 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 PyYAML-5.4.1 absl-py-0.11.0 cachetools-4.2.4 click-7.1.2 cloudpickle-1.6.0 docstring-parser-0.14.1 fire-0.4.0 google-api-python-client-1.12.11 google-auth-1.35.0 google-cloud-storage-1.44.0 jsonschema-3.2.0 kfp-1.6.6 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.2 kubernetes-12.0.1 requests-toolbelt-0.9.1 strip-hints-0.1.10 tabulate-0.8.10 termcolor-1.1.0 uritemplate-3.0.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container f7dd42729af6\n",
      " ---> c785809a007a\n",
      "Step 3/4 : RUN pip install google-cloud-aiplatform==1.3.0\n",
      " ---> Running in adeda393cad9\n",
      "Collecting google-cloud-aiplatform==1.3.0\n",
      "  Downloading google_cloud_aiplatform-1.3.0-py2.py3-none-any.whl (1.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 19.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.3.0) (2.34.4)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.3.0) (1.44.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.3.0) (2.8.1)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.3.0) (1.20.6)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.3.0) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (2.28.1)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (1.35.0)\n",
      "Requirement already satisfied: protobuf<4.0.0dev,>=3.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (3.20.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (1.56.3)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (1.47.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (1.47.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.3.0) (2.3.3)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.3.0) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.3.0) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2.0.0dev,>=1.32.0->google-cloud-aiplatform==1.3.0) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform==1.3.0) (3.0.9)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (59.8.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (4.8)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.3.0) (1.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (2022.6.15)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.3.0) (1.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (0.4.8)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.3.0) (2.21)\n",
      "Installing collected packages: google-cloud-aiplatform\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.15.0\n",
      "    Uninstalling google-cloud-aiplatform-1.15.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.15.0\n",
      "Successfully installed google-cloud-aiplatform-1.3.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container adeda393cad9\n",
      " ---> 401c3ebc0e97\n",
      "Step 4/4 : ENTRYPOINT [\"/bin/bash\"]\n",
      " ---> Running in ecf76bf79bf9\n",
      "Removing intermediate container ecf76bf79bf9\n",
      " ---> b64393be55be\n",
      "Successfully built b64393be55be\n",
      "Successfully tagged gcr.io/qwiklabs-asl-01-1d0b6cba208c/kfp-cli-vertex:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-asl-01-1d0b6cba208c/kfp-cli-vertex:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-asl-01-1d0b6cba208c/kfp-cli-vertex]\n",
      "68f56a7d9e4e: Preparing\n",
      "78b27f8a88d2: Preparing\n",
      "e731d552a12c: Preparing\n",
      "e08fd71ff37a: Preparing\n",
      "e69159dfa907: Preparing\n",
      "ce2f668df2d8: Preparing\n",
      "1c26767a76ae: Preparing\n",
      "7e2f559b3e11: Preparing\n",
      "0511c5f28369: Preparing\n",
      "ec7beeca4cbf: Preparing\n",
      "2c79c2b66f65: Preparing\n",
      "b6700bba959a: Preparing\n",
      "4115677fbd36: Preparing\n",
      "6695713472f0: Preparing\n",
      "0aca33654a88: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "68045cfc6aa2: Preparing\n",
      "c51b40356340: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "20634b178955: Preparing\n",
      "af7ed92504ae: Preparing\n",
      "b6700bba959a: Waiting\n",
      "4115677fbd36: Waiting\n",
      "ce2f668df2d8: Waiting\n",
      "1c26767a76ae: Waiting\n",
      "7e2f559b3e11: Waiting\n",
      "0511c5f28369: Waiting\n",
      "ec7beeca4cbf: Waiting\n",
      "2c79c2b66f65: Waiting\n",
      "6695713472f0: Waiting\n",
      "0aca33654a88: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "68045cfc6aa2: Waiting\n",
      "c51b40356340: Waiting\n",
      "20634b178955: Waiting\n",
      "af7ed92504ae: Waiting\n",
      "e08fd71ff37a: Layer already exists\n",
      "e731d552a12c: Layer already exists\n",
      "e69159dfa907: Layer already exists\n",
      "ce2f668df2d8: Layer already exists\n",
      "1c26767a76ae: Layer already exists\n",
      "7e2f559b3e11: Layer already exists\n",
      "ec7beeca4cbf: Layer already exists\n",
      "0511c5f28369: Layer already exists\n",
      "2c79c2b66f65: Layer already exists\n",
      "b6700bba959a: Layer already exists\n",
      "4115677fbd36: Layer already exists\n",
      "6695713472f0: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "0aca33654a88: Layer already exists\n",
      "68045cfc6aa2: Layer already exists\n",
      "c51b40356340: Layer already exists\n",
      "af7ed92504ae: Layer already exists\n",
      "20634b178955: Layer already exists\n",
      "68f56a7d9e4e: Pushed\n",
      "78b27f8a88d2: Pushed\n",
      "latest: digest: sha256:a24b728333269855cd0f4485a2eaf49ad62a5575184e08824e56e903e4b9dc12 size: 4708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                        STATUS\n",
      "89dc5906-5641-4949-98fd-1de2b15df6cf  2022-07-21T17:25:47+00:00  1M48S     gs://qwiklabs-asl-01-1d0b6cba208c_cloudbuild/source/1658424347.160552-8b2661b97833426789c258fb5f201c03.tgz  gcr.io/qwiklabs-asl-01-1d0b6cba208c/kfp-cli-vertex (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag {KFP_CLI_IMAGE_URI} kfp-cli_vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the **Cloud Build** workflow.\n",
    "\n",
    "Review the `cloudbuild_vertex.yaml` file to understand how the CI/CD workflow is implemented and how environment specific settings are abstracted using **Cloud Build** variables.\n",
    "\n",
    "The CI/CD workflow automates the steps you walked through manually during `lab-02_vertex`:\n",
    "1. Builds the trainer image\n",
    "1. Compiles the pipeline\n",
    "1. Uploads and run the pipeline to the Vertex AI Pipeline environment\n",
    "1. Pushes the trainer to your project's **Container Registry**\n",
    " \n",
    "\n",
    "The **Cloud Build** workflow configuration uses both standard and custom [Cloud Build builders](https://cloud.google.com/cloud-build/docs/cloud-builders). The custom builder encapsulates **KFP CLI**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually triggering CI/CD runs\n",
    "\n",
    "You can manually trigger **Cloud Build** runs using the [gcloud builds submit command]( https://cloud.google.com/sdk/gcloud/reference/builds/submit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_REGION=us-central1,_PIPELINE_FOLDER=./'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBSTITUTIONS = f\"_REGION={REGION},_PIPELINE_FOLDER=./\"\n",
    "SUBSTITUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 12 file(s) totalling 41.0 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://qwiklabs-asl-01-1d0b6cba208c_cloudbuild/source/1658424459.082338-d8d9520963f24d78bcea70a805089a18.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-asl-01-1d0b6cba208c/locations/global/builds/4c2c17f7-188b-40d2-ac5e-65d4398d0e2f].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/4c2c17f7-188b-40d2-ac5e-65d4398d0e2f?project=40028652130].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"4c2c17f7-188b-40d2-ac5e-65d4398d0e2f\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-asl-01-1d0b6cba208c_cloudbuild/source/1658424459.082338-d8d9520963f24d78bcea70a805089a18.tgz#1658424459365533\n",
      "Copying gs://qwiklabs-asl-01-1d0b6cba208c_cloudbuild/source/1658424459.082338-d8d9520963f24d78bcea70a805089a18.tgz#1658424459365533...\n",
      "/ [1 files][  8.3 KiB/  8.3 KiB]                                                \n",
      "Operation completed over 1 objects/8.3 KiB.\n",
      "BUILD\n",
      "Starting Step #0 - \"Build the trainer image\"\n",
      "Step #0 - \"Build the trainer image\": Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #0 - \"Build the trainer image\": Sending build context to Docker daemon  6.144kB\n",
      "Step #0 - \"Build the trainer image\": Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "Step #0 - \"Build the trainer image\": latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "Step #0 - \"Build the trainer image\": d7bfe07ed847: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": 256a16f81350: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": 4f4fb700ef54: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": 1a9e3557648b: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": b32cfad274a4: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": 633b6bf6eafe: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": dc3ebba293d2: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": f14428c2780c: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": 17610ff26dd4: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": c59c59a1007a: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": afa44ccc646d: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": 3202e8e43491: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": 700cfc1617dd: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": 610d6205e6a8: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": 61b77f764283: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": c891c87c58c1: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": 20b2898486c4: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": 1242edd5f479: Pulling fs layer\n",
      "Step #0 - \"Build the trainer image\": c59c59a1007a: Waiting\n",
      "Step #0 - \"Build the trainer image\": afa44ccc646d: Waiting\n",
      "Step #0 - \"Build the trainer image\": 3202e8e43491: Waiting\n",
      "Step #0 - \"Build the trainer image\": 700cfc1617dd: Waiting\n",
      "Step #0 - \"Build the trainer image\": 610d6205e6a8: Waiting\n",
      "Step #0 - \"Build the trainer image\": 61b77f764283: Waiting\n",
      "Step #0 - \"Build the trainer image\": c891c87c58c1: Waiting\n",
      "Step #0 - \"Build the trainer image\": 20b2898486c4: Waiting\n",
      "Step #0 - \"Build the trainer image\": 1242edd5f479: Waiting\n",
      "Step #0 - \"Build the trainer image\": 1a9e3557648b: Waiting\n",
      "Step #0 - \"Build the trainer image\": b32cfad274a4: Waiting\n",
      "Step #0 - \"Build the trainer image\": 633b6bf6eafe: Waiting\n",
      "Step #0 - \"Build the trainer image\": dc3ebba293d2: Waiting\n",
      "Step #0 - \"Build the trainer image\": f14428c2780c: Waiting\n",
      "Step #0 - \"Build the trainer image\": 17610ff26dd4: Waiting\n",
      "Step #0 - \"Build the trainer image\": 256a16f81350: Download complete\n",
      "Step #0 - \"Build the trainer image\": 4f4fb700ef54: Verifying Checksum\n",
      "Step #0 - \"Build the trainer image\": 4f4fb700ef54: Download complete\n",
      "Step #0 - \"Build the trainer image\": d7bfe07ed847: Verifying Checksum\n",
      "Step #0 - \"Build the trainer image\": d7bfe07ed847: Download complete\n",
      "Step #0 - \"Build the trainer image\": 633b6bf6eafe: Verifying Checksum\n",
      "Step #0 - \"Build the trainer image\": 633b6bf6eafe: Download complete\n",
      "Step #0 - \"Build the trainer image\": dc3ebba293d2: Verifying Checksum\n",
      "Step #0 - \"Build the trainer image\": dc3ebba293d2: Download complete\n",
      "Step #0 - \"Build the trainer image\": f14428c2780c: Verifying Checksum\n",
      "Step #0 - \"Build the trainer image\": f14428c2780c: Download complete\n",
      "Step #0 - \"Build the trainer image\": 17610ff26dd4: Verifying Checksum\n",
      "Step #0 - \"Build the trainer image\": 17610ff26dd4: Download complete\n",
      "Step #0 - \"Build the trainer image\": b32cfad274a4: Verifying Checksum\n",
      "Step #0 - \"Build the trainer image\": b32cfad274a4: Download complete\n",
      "Step #0 - \"Build the trainer image\": c59c59a1007a: Verifying Checksum\n",
      "Step #0 - \"Build the trainer image\": c59c59a1007a: Download complete\n",
      "Step #0 - \"Build the trainer image\": afa44ccc646d: Verifying Checksum\n",
      "Step #0 - \"Build the trainer image\": afa44ccc646d: Download complete\n",
      "Step #0 - \"Build the trainer image\": 3202e8e43491: Verifying Checksum\n",
      "Step #0 - \"Build the trainer image\": 3202e8e43491: Download complete\n",
      "Step #0 - \"Build the trainer image\": 700cfc1617dd: Verifying Checksum\n",
      "Step #0 - \"Build the trainer image\": 700cfc1617dd: Download complete\n",
      "Step #0 - \"Build the trainer image\": 610d6205e6a8: Verifying Checksum\n",
      "Step #0 - \"Build the trainer image\": 610d6205e6a8: Download complete\n",
      "Step #0 - \"Build the trainer image\": 61b77f764283: Download complete\n",
      "Step #0 - \"Build the trainer image\": c891c87c58c1: Verifying Checksum\n",
      "Step #0 - \"Build the trainer image\": c891c87c58c1: Download complete\n",
      "Step #0 - \"Build the trainer image\": 1242edd5f479: Verifying Checksum\n",
      "Step #0 - \"Build the trainer image\": 1242edd5f479: Download complete\n",
      "Step #0 - \"Build the trainer image\": 1a9e3557648b: Verifying Checksum\n",
      "Step #0 - \"Build the trainer image\": 1a9e3557648b: Download complete\n",
      "Step #0 - \"Build the trainer image\": d7bfe07ed847: Pull complete\n",
      "Step #0 - \"Build the trainer image\": 256a16f81350: Pull complete\n",
      "Step #0 - \"Build the trainer image\": 4f4fb700ef54: Pull complete\n",
      "Step #0 - \"Build the trainer image\": 20b2898486c4: Verifying Checksum\n",
      "Step #0 - \"Build the trainer image\": 20b2898486c4: Download complete\n",
      "Step #0 - \"Build the trainer image\": 1a9e3557648b: Pull complete\n",
      "Step #0 - \"Build the trainer image\": b32cfad274a4: Pull complete\n",
      "Step #0 - \"Build the trainer image\": 633b6bf6eafe: Pull complete\n",
      "Step #0 - \"Build the trainer image\": dc3ebba293d2: Pull complete\n",
      "Step #0 - \"Build the trainer image\": f14428c2780c: Pull complete\n",
      "Step #0 - \"Build the trainer image\": 17610ff26dd4: Pull complete\n",
      "Step #0 - \"Build the trainer image\": c59c59a1007a: Pull complete\n",
      "Step #0 - \"Build the trainer image\": afa44ccc646d: Pull complete\n",
      "Step #0 - \"Build the trainer image\": 3202e8e43491: Pull complete\n",
      "Step #0 - \"Build the trainer image\": 700cfc1617dd: Pull complete\n",
      "Step #0 - \"Build the trainer image\": 610d6205e6a8: Pull complete\n",
      "Step #0 - \"Build the trainer image\": 61b77f764283: Pull complete\n",
      "Step #0 - \"Build the trainer image\": c891c87c58c1: Pull complete\n",
      "Step #0 - \"Build the trainer image\": 20b2898486c4: Pull complete\n",
      "Step #0 - \"Build the trainer image\": 1242edd5f479: Pull complete\n",
      "Step #0 - \"Build the trainer image\": Digest: sha256:fd1a6b82332194835f0dd686b8b9e8f8e372919e0e83e6663160b823dd65e194\n",
      "Step #0 - \"Build the trainer image\": Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      "Step #0 - \"Build the trainer image\":  ---> 234ba2bc2b77\n",
      "Step #0 - \"Build the trainer image\": Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      "Step #0 - \"Build the trainer image\":  ---> Running in e04e1da778d5\n",
      "Step #0 - \"Build the trainer image\": Collecting fire\n",
      "Step #0 - \"Build the trainer image\":   Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Step #0 - \"Build the trainer image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 kB 4.7 MB/s eta 0:00:00\n",
      "Step #0 - \"Build the trainer image\":   Preparing metadata (setup.py): started\n",
      "Step #0 - \"Build the trainer image\":   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0 - \"Build the trainer image\": Collecting cloudml-hypertune\n",
      "Step #0 - \"Build the trainer image\":   Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Step #0 - \"Build the trainer image\":   Preparing metadata (setup.py): started\n",
      "Step #0 - \"Build the trainer image\":   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0 - \"Build the trainer image\": Collecting scikit-learn==0.20.4\n",
      "Step #0 - \"Build the trainer image\":   Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Step #0 - \"Build the trainer image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 37.9 MB/s eta 0:00:00\n",
      "Step #0 - \"Build the trainer image\": Collecting pandas==0.24.2\n",
      "Step #0 - \"Build the trainer image\":   Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Step #0 - \"Build the trainer image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 43.2 MB/s eta 0:00:00\n",
      "Step #0 - \"Build the trainer image\": Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.7.3)\n",
      "Step #0 - \"Build the trainer image\": Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.19.5)\n",
      "Step #0 - \"Build the trainer image\": Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.2)\n",
      "Step #0 - \"Build the trainer image\": Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2022.1)\n",
      "Step #0 - \"Build the trainer image\": Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Step #0 - \"Build the trainer image\": Collecting termcolor\n",
      "Step #0 - \"Build the trainer image\":   Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Step #0 - \"Build the trainer image\":   Preparing metadata (setup.py): started\n",
      "Step #0 - \"Build the trainer image\":   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0 - \"Build the trainer image\": Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "Step #0 - \"Build the trainer image\":   Building wheel for fire (setup.py): started\n",
      "Step #0 - \"Build the trainer image\":   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #0 - \"Build the trainer image\":   Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=59ded076080e1ef50da34692a8f196cbaa9ce624a6e342679806790f8a2242ce\n",
      "Step #0 - \"Build the trainer image\":   Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "Step #0 - \"Build the trainer image\":   Building wheel for cloudml-hypertune (setup.py): started\n",
      "Step #0 - \"Build the trainer image\":   Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "Step #0 - \"Build the trainer image\":   Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=70fb77aa5dc3a301a5c47261ed093813836c4d1738a6d71c6cb2eb040af19119\n",
      "Step #0 - \"Build the trainer image\":   Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Step #0 - \"Build the trainer image\":   Building wheel for termcolor (setup.py): started\n",
      "Step #0 - \"Build the trainer image\":   Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "Step #0 - \"Build the trainer image\":   Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=423ccee3c47d8593cbeb4e73dbb2cc3f4bf395b3a916b7994b840874a466e61b\n",
      "Step #0 - \"Build the trainer image\":   Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Step #0 - \"Build the trainer image\": Successfully built fire cloudml-hypertune termcolor\n",
      "Step #0 - \"Build the trainer image\": Installing collected packages: termcolor, cloudml-hypertune, fire, scikit-learn, pandas\n",
      "Step #0 - \"Build the trainer image\":   Attempting uninstall: scikit-learn\n",
      "Step #0 - \"Build the trainer image\":     Found existing installation: scikit-learn 1.0.2\n",
      "Step #0 - \"Build the trainer image\":     Uninstalling scikit-learn-1.0.2:\n",
      "Step #0 - \"Build the trainer image\":       Successfully uninstalled scikit-learn-1.0.2\n",
      "Step #0 - \"Build the trainer image\":   Attempting uninstall: pandas\n",
      "Step #0 - \"Build the trainer image\":     Found existing installation: pandas 1.3.5\n",
      "Step #0 - \"Build the trainer image\":     Uninstalling pandas-1.3.5:\n",
      "Step #0 - \"Build the trainer image\":       Successfully uninstalled pandas-1.3.5\n",
      "Step #0 - \"Build the trainer image\": \u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "Step #0 - \"Build the trainer image\": visions 0.7.4 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #0 - \"Build the trainer image\": statsmodels 0.13.2 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #0 - \"Build the trainer image\": phik 0.12.2 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #0 - \"Build the trainer image\": pandas-profiling 3.2.0 requires pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #0 - \"Build the trainer image\": \u001b[0mSuccessfully installed cloudml-hypertune-0.1.0.dev6 fire-0.4.0 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Step #0 - \"Build the trainer image\": \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #0 - \"Build the trainer image\": \u001b[0mRemoving intermediate container e04e1da778d5\n",
      "Step #0 - \"Build the trainer image\":  ---> 3248a79cf63b\n",
      "Step #0 - \"Build the trainer image\": Step 3/5 : WORKDIR /app\n",
      "Step #0 - \"Build the trainer image\":  ---> Running in a7ed59cf7524\n",
      "Step #0 - \"Build the trainer image\": Removing intermediate container a7ed59cf7524\n",
      "Step #0 - \"Build the trainer image\":  ---> e28df1cc5885\n",
      "Step #0 - \"Build the trainer image\": Step 4/5 : COPY train.py .\n",
      "Step #0 - \"Build the trainer image\":  ---> 8aa47d485d40\n",
      "Step #0 - \"Build the trainer image\": Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      "Step #0 - \"Build the trainer image\":  ---> Running in 13b731811477\n",
      "Step #0 - \"Build the trainer image\": Removing intermediate container 13b731811477\n",
      "Step #0 - \"Build the trainer image\":  ---> 8b591d4921ab\n",
      "Step #0 - \"Build the trainer image\": Successfully built 8b591d4921ab\n",
      "Step #0 - \"Build the trainer image\": Successfully tagged gcr.io/qwiklabs-asl-01-1d0b6cba208c/trainer_image_covertype_vertex:latest\n",
      "Finished Step #0 - \"Build the trainer image\"\n",
      "Starting Step #1 - \"Push the trainer image\"\n",
      "Step #1 - \"Push the trainer image\": Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #1 - \"Push the trainer image\": The push refers to repository [gcr.io/qwiklabs-asl-01-1d0b6cba208c/trainer_image_covertype_vertex]\n",
      "Step #1 - \"Push the trainer image\": 522fde2963ae: Preparing\n",
      "Step #1 - \"Push the trainer image\": 85303e882215: Preparing\n",
      "Step #1 - \"Push the trainer image\": b9dbccad0a10: Preparing\n",
      "Step #1 - \"Push the trainer image\": e731d552a12c: Preparing\n",
      "Step #1 - \"Push the trainer image\": e08fd71ff37a: Preparing\n",
      "Step #1 - \"Push the trainer image\": e69159dfa907: Preparing\n",
      "Step #1 - \"Push the trainer image\": ce2f668df2d8: Preparing\n",
      "Step #1 - \"Push the trainer image\": 1c26767a76ae: Preparing\n",
      "Step #1 - \"Push the trainer image\": 7e2f559b3e11: Preparing\n",
      "Step #1 - \"Push the trainer image\": 0511c5f28369: Preparing\n",
      "Step #1 - \"Push the trainer image\": ec7beeca4cbf: Preparing\n",
      "Step #1 - \"Push the trainer image\": 2c79c2b66f65: Preparing\n",
      "Step #1 - \"Push the trainer image\": b6700bba959a: Preparing\n",
      "Step #1 - \"Push the trainer image\": 4115677fbd36: Preparing\n",
      "Step #1 - \"Push the trainer image\": 6695713472f0: Preparing\n",
      "Step #1 - \"Push the trainer image\": 0aca33654a88: Preparing\n",
      "Step #1 - \"Push the trainer image\": 5f70bf18a086: Preparing\n",
      "Step #1 - \"Push the trainer image\": 68045cfc6aa2: Preparing\n",
      "Step #1 - \"Push the trainer image\": c51b40356340: Preparing\n",
      "Step #1 - \"Push the trainer image\": 5f70bf18a086: Preparing\n",
      "Step #1 - \"Push the trainer image\": 20634b178955: Preparing\n",
      "Step #1 - \"Push the trainer image\": af7ed92504ae: Preparing\n",
      "Step #1 - \"Push the trainer image\": e69159dfa907: Waiting\n",
      "Step #1 - \"Push the trainer image\": ce2f668df2d8: Waiting\n",
      "Step #1 - \"Push the trainer image\": 1c26767a76ae: Waiting\n",
      "Step #1 - \"Push the trainer image\": 2c79c2b66f65: Waiting\n",
      "Step #1 - \"Push the trainer image\": 7e2f559b3e11: Waiting\n",
      "Step #1 - \"Push the trainer image\": 0511c5f28369: Waiting\n",
      "Step #1 - \"Push the trainer image\": ec7beeca4cbf: Waiting\n",
      "Step #1 - \"Push the trainer image\": b6700bba959a: Waiting\n",
      "Step #1 - \"Push the trainer image\": 4115677fbd36: Waiting\n",
      "Step #1 - \"Push the trainer image\": 6695713472f0: Waiting\n",
      "Step #1 - \"Push the trainer image\": 0aca33654a88: Waiting\n",
      "Step #1 - \"Push the trainer image\": 5f70bf18a086: Waiting\n",
      "Step #1 - \"Push the trainer image\": 68045cfc6aa2: Waiting\n",
      "Step #1 - \"Push the trainer image\": c51b40356340: Waiting\n",
      "Step #1 - \"Push the trainer image\": 20634b178955: Waiting\n",
      "Step #1 - \"Push the trainer image\": af7ed92504ae: Waiting\n",
      "Step #1 - \"Push the trainer image\": e08fd71ff37a: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": e731d552a12c: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": e69159dfa907: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": ce2f668df2d8: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": 7e2f559b3e11: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": 1c26767a76ae: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": 0511c5f28369: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": ec7beeca4cbf: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": b6700bba959a: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": 2c79c2b66f65: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": 4115677fbd36: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": 6695713472f0: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": 0aca33654a88: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": 5f70bf18a086: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": c51b40356340: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": 68045cfc6aa2: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": 20634b178955: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": af7ed92504ae: Layer already exists\n",
      "Step #1 - \"Push the trainer image\": 85303e882215: Pushed\n",
      "Step #1 - \"Push the trainer image\": 522fde2963ae: Pushed\n",
      "Step #1 - \"Push the trainer image\": b9dbccad0a10: Pushed\n",
      "Step #1 - \"Push the trainer image\": latest: digest: sha256:11f15ff5a13b0b3f7ddc52c2eb0f8d45fb2f7cd2856776e16e1fe2c4ea3a723f size: 4913\n",
      "Finished Step #1 - \"Push the trainer image\"\n",
      "Starting Step #2 - \"Compile the pipeline\"\n",
      "Step #2 - \"Compile the pipeline\": Pulling image: gcr.io/qwiklabs-asl-01-1d0b6cba208c/kfp-cli-vertex\n",
      "Step #2 - \"Compile the pipeline\": Using default tag: latest\n",
      "Step #2 - \"Compile the pipeline\": latest: Pulling from qwiklabs-asl-01-1d0b6cba208c/kfp-cli-vertex\n",
      "Step #2 - \"Compile the pipeline\": d7bfe07ed847: Already exists\n",
      "Step #2 - \"Compile the pipeline\": 256a16f81350: Already exists\n",
      "Step #2 - \"Compile the pipeline\": 4f4fb700ef54: Already exists\n",
      "Step #2 - \"Compile the pipeline\": 1a9e3557648b: Already exists\n",
      "Step #2 - \"Compile the pipeline\": b32cfad274a4: Already exists\n",
      "Step #2 - \"Compile the pipeline\": 4f4fb700ef54: Already exists\n",
      "Step #2 - \"Compile the pipeline\": 633b6bf6eafe: Already exists\n",
      "Step #2 - \"Compile the pipeline\": dc3ebba293d2: Already exists\n",
      "Step #2 - \"Compile the pipeline\": f14428c2780c: Already exists\n",
      "Step #2 - \"Compile the pipeline\": 17610ff26dd4: Already exists\n",
      "Step #2 - \"Compile the pipeline\": c59c59a1007a: Already exists\n",
      "Step #2 - \"Compile the pipeline\": afa44ccc646d: Already exists\n",
      "Step #2 - \"Compile the pipeline\": 3202e8e43491: Already exists\n",
      "Step #2 - \"Compile the pipeline\": 700cfc1617dd: Already exists\n",
      "Step #2 - \"Compile the pipeline\": 610d6205e6a8: Already exists\n",
      "Step #2 - \"Compile the pipeline\": 61b77f764283: Already exists\n",
      "Step #2 - \"Compile the pipeline\": c891c87c58c1: Already exists\n",
      "Step #2 - \"Compile the pipeline\": 20b2898486c4: Already exists\n",
      "Step #2 - \"Compile the pipeline\": 1242edd5f479: Already exists\n",
      "Step #2 - \"Compile the pipeline\": 21953cdb69c6: Pulling fs layer\n",
      "Step #2 - \"Compile the pipeline\": b577160cea80: Pulling fs layer\n",
      "Step #2 - \"Compile the pipeline\": b577160cea80: Verifying Checksum\n",
      "Step #2 - \"Compile the pipeline\": b577160cea80: Download complete\n",
      "Step #2 - \"Compile the pipeline\": 21953cdb69c6: Verifying Checksum\n",
      "Step #2 - \"Compile the pipeline\": 21953cdb69c6: Download complete\n",
      "Step #2 - \"Compile the pipeline\": 21953cdb69c6: Pull complete\n",
      "Step #2 - \"Compile the pipeline\": b577160cea80: Pull complete\n",
      "Step #2 - \"Compile the pipeline\": Digest: sha256:a24b728333269855cd0f4485a2eaf49ad62a5575184e08824e56e903e4b9dc12\n",
      "Step #2 - \"Compile the pipeline\": Status: Downloaded newer image for gcr.io/qwiklabs-asl-01-1d0b6cba208c/kfp-cli-vertex:latest\n",
      "Step #2 - \"Compile the pipeline\": gcr.io/qwiklabs-asl-01-1d0b6cba208c/kfp-cli-vertex:latest\n",
      "Finished Step #2 - \"Compile the pipeline\"\n",
      "Starting Step #3 - \"Run the pipeline\"\n",
      "Step #3 - \"Run the pipeline\": Already have image (with digest): gcr.io/qwiklabs-asl-01-1d0b6cba208c/kfp-cli-vertex\n",
      "Step #3 - \"Run the pipeline\": INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "Step #3 - \"Run the pipeline\": INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/40028652130/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20220721173002\n",
      "Step #3 - \"Run the pipeline\": INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "Step #3 - \"Run the pipeline\": INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/40028652130/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20220721173002')\n",
      "Step #3 - \"Run the pipeline\": INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "Step #3 - \"Run the pipeline\": https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/covertype-kfp-pipeline-20220721173002?project=40028652130\n",
      "Step #3 - \"Run the pipeline\": INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40028652130/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20220721173002 current state:\n",
      "Step #3 - \"Run the pipeline\": PipelineState.PIPELINE_STATE_RUNNING\n",
      "Step #3 - \"Run the pipeline\": INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40028652130/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20220721173002 current state:\n",
      "Step #3 - \"Run the pipeline\": PipelineState.PIPELINE_STATE_RUNNING\n",
      "Step #3 - \"Run the pipeline\": INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40028652130/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20220721173002 current state:\n",
      "Step #3 - \"Run the pipeline\": PipelineState.PIPELINE_STATE_RUNNING\n",
      "Step #3 - \"Run the pipeline\": INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40028652130/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20220721173002 current state:\n",
      "Step #3 - \"Run the pipeline\": PipelineState.PIPELINE_STATE_RUNNING\n",
      "Step #3 - \"Run the pipeline\": INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40028652130/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20220721173002 current state:\n",
      "Step #3 - \"Run the pipeline\": PipelineState.PIPELINE_STATE_RUNNING\n",
      "Step #3 - \"Run the pipeline\": INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/40028652130/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20220721173002 current state:\n",
      "Step #3 - \"Run the pipeline\": PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit . --config cloudbuild_vertex.yaml --substitutions {SUBSTITUTIONS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you experience issues with CloudBuild being able to access Vertex AI, you may need to run the following commands in **CloudShell**:\n",
    "\n",
    "```\n",
    "PROJECT_ID=$(gcloud config get-value project)\n",
    "PROJECT_NUMBER=$(gcloud projects list --filter=\"name=$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\")\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=\"serviceAccount:$PROJECT_NUMBER@cloudbuild.gserviceaccount.com\" \\\n",
    "    --role=\"roles/aiplatform.user\"\n",
    "gcloud iam service-accounts add-iam-policy-binding \\\n",
    "    $PROJECT_NUMBER-compute@developer.gserviceaccount.com \\\n",
    "    --member=\"serviceAccount:$PROJECT_NUMBER@cloudbuild.gserviceaccount.com\" \\\n",
    "    --role=\"roles/iam.serviceAccountUser\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up GitHub integration\n",
    "\n",
    "In this exercise you integrate your CI/CD workflow with **GitHub**, using [Cloud Build GitHub App](https://github.com/marketplace/google-cloud-build). \n",
    "You will set up a trigger that starts the CI/CD workflow when a new tag is applied to the **GitHub** repo managing the  pipeline source code. You will use a fork of this repo as your source GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a fork of this repo\n",
    "[Follow the GitHub documentation](https://help.github.com/en/github/getting-started-with-github/fork-a-repo) to fork [this repo](https://github.com/GoogleCloudPlatform/asl-ml-immersion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a **Cloud Build** trigger\n",
    "\n",
    "Connect the fork you created in the previous step to your Google Cloud project and create a trigger following the steps in the [Creating GitHub app trigger](https://cloud.google.com/cloud-build/docs/create-github-app-triggers) article. Use the following values on the **Edit trigger** form:\n",
    "\n",
    "|Field|Value|\n",
    "|-----|-----|\n",
    "|Name|[YOUR TRIGGER NAME]|\n",
    "|Description|[YOUR TRIGGER DESCRIPTION]|\n",
    "|Event| Tag|\n",
    "|Source| [YOUR FORK]|\n",
    "|Tag (regex)|.\\*|\n",
    "|Build Configuration|Cloud Build configuration file (yaml or json)|\n",
    "|Cloud Build configuration file location| ./notebooks/kubeflow_pipelines/cicd/solutions/cloudbuild_vertex.yaml|\n",
    "\n",
    "\n",
    "Use the following values for the substitution variables:\n",
    "\n",
    "|Variable|Value|\n",
    "|--------|-----|\n",
    "|_REGION|us-central1|\n",
    "|_PIPELINE_FOLDER|notebooks/kubeflow_pipelines/cicd/solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger the build\n",
    "\n",
    "To start an automated build [create a new release of the repo in GitHub](https://help.github.com/en/github/administering-a-repository/creating-releases). Alternatively, you can start the build by applying a tag using `git`. \n",
    "```\n",
    "git tag [TAG NAME]\n",
    "git push origin --tags\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the command above, a build should have been automatically triggered, which you should able to inspect [here](https://console.cloud.google.com/cloud-build/builds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
